{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following analysis aims to understand the impact of failures of active validators to timely upgrade to the Delphi protocol version on Tezos network.  \n",
    "\n",
    "We applied the following assumptions to the analysis:\n",
    "(1) Bakers that failed to endorse more than two slots and did not recover from the failure within two hours of the upgrade were considered as impacted by the upgrade.\n",
    "(2) Active bakers were calculated as the total number of bakers who had endorsing rights in the five cycles immediately after the upgrade (cycles 296..300) and were active in the network with average reliability of 20% or higher before the upgrade.\n",
    "(3) Bakers who had an average reliability of less than 20% during the four cycles before the upgrade were excluded from the analysis.\n",
    "(4) Bakers who started their operations in the cycle that followed the upgrade were excluded from the analysis.\n",
    "(5) The analysis only considered data for the four cycles before the upgrade and five cycles after the upgrade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import datetime\n",
    "\n",
    "# Function to convert datetime in milliseconds to human readable format\n",
    "def convert_from_ms( milliseconds ): \n",
    "    seconds, milliseconds = divmod(milliseconds,1000) \n",
    "    minutes, seconds = divmod(seconds, 60) \n",
    "    hours, minutes = divmod(minutes, 60) \n",
    "    days, hours = divmod(hours, 24) \n",
    "    seconds = seconds + milliseconds/1000 \n",
    "    return round(((days*24*60+hours*60+minutes)/60),2)\n",
    "\n",
    "# Upgrade cycle parameters \n",
    "cycle_upgrade = 296\n",
    "cycle_before_upgrade_range = [\n",
    "    cycle_upgrade-4,\n",
    "    cycle_upgrade-3,\n",
    "    cycle_upgrade-2,\n",
    "    cycle_upgrade-1\n",
    "    ]\n",
    "cycle_rights_upgrade_range = [\n",
    "    cycle_upgrade,\n",
    "    cycle_upgrade+1,\n",
    "    cycle_upgrade+2,\n",
    "    cycle_upgrade+3,\n",
    "    cycle_upgrade+4,\n",
    "    ]\n",
    "cycle_upgrade_range = [\n",
    "    cycle_upgrade-4,\n",
    "    cycle_upgrade-3,\n",
    "    cycle_upgrade-2,\n",
    "    cycle_upgrade-1,\n",
    "    cycle_upgrade,\n",
    "    cycle_upgrade+1,\n",
    "    cycle_upgrade+2,\n",
    "    cycle_upgrade+3,\n",
    "    cycle_upgrade+4\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get upgrade block start height and datetime and end block height, timedate\n",
    "cycle_json = requests.get(\"https://api.tzstats.com/explorer/cycle/%s\" % cycle_upgrade)\n",
    "cycle = cycle_json.json()\n",
    "cycle_start_height = cycle.get(\"start_height\")\n",
    "cycle_start_datetime = cycle.get(\"start_time\")\n",
    "cycle_start_datetime = datetime.datetime.strptime(cycle_start_datetime, '%Y-%m-%dT%H:%M:%fZ')\n",
    "cycle_start_datetime = int((cycle_start_datetime - datetime.datetime(1970, 1, 1)).total_seconds()*1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Income table API to retrieve metrics per cycle per baker\n",
    "income_dict = {}\n",
    "for cycle_income in cycle_upgrade_range:\n",
    "    income_json = requests.get(\"https://api.tzstats.com/tables/income?&columns=cycle,address,row_id,rolls,n_delegations,contribution_percent,n_blocks_baked,n_blocks_lost,n_slots_endorsed,n_slots_missed,fees_income,n_baking_rights,n_endorsing_rights,balance,delegated,missed_baking_income,missed_endorsing_income&cycle=%s\" % cycle_income)\n",
    "    income = income_json.json()\n",
    "    for baker in income:\n",
    "        if baker[1] not in income_dict:\n",
    "            income_dict[baker[1]]={}\n",
    "        income_dict[baker[1]][int(baker[0])]={\n",
    "            \"rolls\":baker[3],\n",
    "            \"n_delegations\":baker[4],\n",
    "            \"contribution_percent\":baker[5],\n",
    "            \"n_blocks_baked\":baker[6],\n",
    "            \"n_blocks_lost\":baker[7],\n",
    "            \"n_slots_endorsed\":baker[8],\n",
    "            \"n_slots_missed\":baker[9],\n",
    "            \"fees_income\":baker[10],\n",
    "            \"n_baking_rights\":baker[11],\n",
    "            \"n_endorsing_rights\":baker[12],\n",
    "            \"baker_balance\":baker[13], #A bakerâ€™s own balance, composed of spendable balance and frozen deposits plus frozen fees (at snapshot block). Note that frozen rewards do not contribute towards rolls, hence they are not part of balance here.\n",
    "            \"delegated_balance\":baker[14], #Delegated balance (at snapshot block).\n",
    "            \"missed_baking_income\":baker[15],\n",
    "            \"missed_endorsing_income\":baker[16]\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call Rigths API to retrive blocks stats within selected cycle\n",
    "def query_all_api_rows(cycle):\n",
    "    rights = []\n",
    "    cursor = 0\n",
    "    while True:\n",
    "        current_row_json = requests.get(\"https://api.tzstats.com/tables/rights?cycle=%s&type=endorsing&columns=address,height,is_missed,time,priority,cycle,type,row_id&cursor=%s&limit=500000\" %(cycle,cursor))\n",
    "        current_row = current_row_json.json()\n",
    "        if current_row == []:\n",
    "            break\n",
    "        rights.extend(current_row)\n",
    "        cursor = current_row[-1][7]\n",
    "    return rights\n",
    "\n",
    "baker_dict = {}\n",
    "for cycle in cycle_rights_upgrade_range:\n",
    "    rights = query_all_api_rows(cycle)\n",
    "    for right in rights:\n",
    "        if right[0] not in baker_dict:\n",
    "            baker_dict[right[0]] = {}\n",
    "        if int(right[1]) not in baker_dict[right[0]]:\n",
    "            baker_dict[right[0]][int(right[1])] = {}\n",
    "        baker_dict[right[0]][int(right[1])][right[4]] = {\n",
    "            \"cycle\":right[5],\n",
    "            \"success\":(True if right[2] == 0 else False),\n",
    "            \"datetime\":right[3]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determine total number of lost blocks and missed endorsements during the five cycles after the upgrade\n",
    "total_endorsements ={baker_k: \n",
    "    {'total_blocks_endorsed':len(baker_v), \n",
    "    'total_slots_endorsed':sum([ len(slot.keys()) \n",
    "    for slot in baker_v.values()])} \n",
    "    for (baker_k,baker_v) in baker_dict.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary for baker realiability per cycle\n",
    "cycle_reliability_upgrade = {\n",
    "    baker_k: {\n",
    "        cycle_k: cycle_v['contribution_percent'] for cycle_k, cycle_v in baker_v.items()\n",
    "        } for baker_k,baker_v in income_dict.items()}\n",
    "cycle_reliability_upgrade = { k:v for k,v in cycle_reliability_upgrade.items() if v != {}}\n",
    "\n",
    "# Calculate average reliability per baker for the three cycles immediately before the upgrade cycle\n",
    "cycle_reliability_before_upgrade_dict = {}\n",
    "for baker_key, baker_val in cycle_reliability_upgrade.items():\n",
    "    contributions_before_upgrade = [ baker_val[cycle_before_upgrade] for cycle_before_upgrade in cycle_before_upgrade_range if cycle_before_upgrade in baker_val and income_dict[baker_key][cycle_before_upgrade]['n_baking_rights']+income_dict[baker_key][cycle_before_upgrade]['n_endorsing_rights']!=0]\n",
    "    if len(contributions_before_upgrade)>0:\n",
    "        average_contribution_before_upgrade = round(sum(contributions_before_upgrade) / len(contributions_before_upgrade),2)\n",
    "        if average_contribution_before_upgrade > 20:\n",
    "            cycle_reliability_before_upgrade_dict[baker_key] = average_contribution_before_upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get private vs public attribute for bakers\n",
    "account_json = requests.get(\"https://api.tzkt.io/v1/delegates/?select=address,alias,balance&limit=10000\")\n",
    "is_baker_public ={account['address']:(account['alias'] != None) for account in account_json.json()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analyse correlation between voting and succesful upgrade of the operations to a new protocol\n",
    "#Delphi: proposal_id = 8\n",
    "ballots_json = requests.get(\"https://api.tzstats.com/tables/ballot?proposal_id=8\")\n",
    "baker_ballot_dict = {ballot[11]:True for ballot in ballots_json.json() if ballot[4]=='promotion_vote'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate upgrade reliability results per baker in a separate dictionary\n",
    "upgrade_results = {}\n",
    "count_bakers_more_than_cycle_to_fix = 0\n",
    "for baker_key in cycle_reliability_before_upgrade_dict.keys():\n",
    "    if baker_key in baker_dict:\n",
    "        baker_val = baker_dict[baker_key]\n",
    "        time_first_success = 0\n",
    "        first_success = 0\n",
    "        upgrade_failures = 0 \n",
    "        upgrade_results[baker_key] = {\n",
    "                'public':is_baker_public[baker_key],\n",
    "                'rolls_upgrade_cycle':income_dict[baker_key][cycle_upgrade]['rolls'] if cycle_upgrade in income_dict[baker_key] else 0,\n",
    "                'baker_balance':income_dict[baker_key][cycle_upgrade]['baker_balance'] if cycle_upgrade in income_dict[baker_key] else 0,\n",
    "                'delegated_balance':income_dict[baker_key][cycle_upgrade]['delegated_balance'] if cycle_upgrade in income_dict[baker_key] else 0,\n",
    "                'delegations_upgrade_cycle':income_dict[baker_key][cycle_upgrade]['n_delegations'] if cycle_upgrade in income_dict[baker_key] else 0,\n",
    "                'avg_contribution_before_upgrade':cycle_reliability_before_upgrade_dict[baker_key],\n",
    "                'n_rights_upgrade_cycle':(income_dict[baker_key][cycle_upgrade]['n_baking_rights']+income_dict[baker_key][cycle_upgrade]['n_endorsing_rights']) if cycle_upgrade in income_dict[baker_key] else 0,\n",
    "                'contribution_upgrade_cycle':( cycle_reliability_upgrade[baker_key][cycle_upgrade] if income_dict[baker_key][cycle_upgrade]['n_baking_rights']+income_dict[baker_key][cycle_upgrade]['n_endorsing_rights']!=0 else 0) if cycle_upgrade in income_dict[baker_key] else 0,\n",
    "                'n_slots_missed_due_to_upgrade_issues':0, #after the upgrade until the first successful bake or until the end of upgrade_cycle+4\n",
    "                'total_slots_endorsed':total_endorsements[baker_key]['total_slots_endorsed'], #in upgrade cycle and 4 cycles after the upgrade\n",
    "                'total_blocks_endorsed':total_endorsements[baker_key]['total_blocks_endorsed'], #in upgrade cycle and 4 cycles after the upgrade   \n",
    "                'last_block_failed_to_endorse':0,\n",
    "                'first_successful_block':0, \n",
    "                'time_to_fix_hours':0,  \n",
    "                'voted': baker_ballot_dict[baker_key] if baker_key in baker_ballot_dict.keys() else False,\n",
    "                'upgrade_ready': True\n",
    "                }\n",
    "        for height in sorted(baker_val.keys()):\n",
    "# bakers who failed to bake the first block after the upgrade\n",
    "            if next(iter(baker_val[height].values()))['success']==True:\n",
    "                time_first_success = next(iter(baker_val[height].values()))[\"datetime\"]\n",
    "                first_success = height  \n",
    "                upgrade_results[baker_key].update({'first_successful_block': first_success})\n",
    "                break\n",
    "            else:\n",
    "                upgrade_failures += 1\n",
    "                max_failure = height\n",
    "                time_last_failure = next(iter(baker_val[max_failure].values()))[\"datetime\"]\n",
    "        if upgrade_failures > 1 and (cycle_upgrade in income_dict[baker_key]):\n",
    "            time_to_fix = convert_from_ms(time_first_success - cycle_start_datetime)\n",
    "            upgrade_results[baker_key].update({'last_block_failed_to_endorse': max_failure})\n",
    "            upgrade_results[baker_key].update({'time_to_fix_hours':time_to_fix})\n",
    "            upgrade_results[baker_key].update({'n_slots_missed_due_to_upgrade_issues':upgrade_failures}) #after the upgrade until the first successful bake or until the end of upgrade_cycle+4\n",
    "            if time_to_fix<0 or time_to_fix>2:\n",
    "                upgrade_results[baker_key].update({'upgrade_ready':False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Organize upgrade results in a table compatible with Pandas DataFrame\n",
    "upgrade_results_pn = {'address':[],\n",
    "                      'public':[],\n",
    "                      'rolls_upgrade_cycle':[],\n",
    "                      'baker_balance':[],\n",
    "                      'delegated_balance':[], \n",
    "                      'delegations_upgrade_cycle':[],\n",
    "                      'n_rights_upgrade_cycle':[],\n",
    "                      'avg_contribution_before_upgrade':[],\n",
    "                      'contribution_upgrade_cycle':[],\n",
    "                      'n_slots_missed_due_to_upgrade_issues':[],\n",
    "                      'total_slots':[],\n",
    "                      'total_blocks':[],\n",
    "                      'last_block_failed_to_endorse':[],\n",
    "                      'first_successful_block':[],\n",
    "                      'time_to_fix_hours':[],\n",
    "                      'voted':[],\n",
    "                      'upgrade_ready':[]\n",
    "                       }\n",
    "# Populate upgrade reliability results per baker in a separate dictionary\n",
    "for k,v in upgrade_results.items():\n",
    "    upgrade_results_pn['address'].append(k)\n",
    "    upgrade_results_pn['public'].append(v['public'])\n",
    "    upgrade_results_pn['rolls_upgrade_cycle'].append(v['rolls_upgrade_cycle'])\n",
    "    upgrade_results_pn['baker_balance'].append(v['baker_balance'])\n",
    "    upgrade_results_pn['delegated_balance'].append(v['delegated_balance'])\n",
    "    upgrade_results_pn['delegations_upgrade_cycle'].append(v['delegations_upgrade_cycle'])\n",
    "    upgrade_results_pn['n_rights_upgrade_cycle'].append(v['n_rights_upgrade_cycle'])\n",
    "    upgrade_results_pn['avg_contribution_before_upgrade'].append(v['avg_contribution_before_upgrade'])\n",
    "    upgrade_results_pn['contribution_upgrade_cycle'].append(v['contribution_upgrade_cycle'])    \n",
    "    upgrade_results_pn['n_slots_missed_due_to_upgrade_issues'].append(v['n_slots_missed_due_to_upgrade_issues'])\n",
    "    upgrade_results_pn['total_slots'].append(v['total_slots_endorsed'])\n",
    "    upgrade_results_pn['total_blocks'].append(v['total_blocks_endorsed'])\n",
    "    upgrade_results_pn['last_block_failed_to_endorse'].append(v['last_block_failed_to_endorse'])\n",
    "    upgrade_results_pn['first_successful_block'].append(v['first_successful_block'])\n",
    "    upgrade_results_pn['time_to_fix_hours'].append(v['time_to_fix_hours'])\n",
    "    upgrade_results_pn['voted'].append(v['voted'])\n",
    "    upgrade_results_pn['upgrade_ready'].append(v['upgrade_ready'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Calculate time it took to fix public baker operations after initial failure following the upgrade\n",
    "bakers_fixed_within_5cycles_public = {\n",
    "                    'rolls_upgrade_cycle':[],\n",
    "                    'time_to_fix_hours':[]\n",
    "                    }\n",
    "for k,v in upgrade_results.items():\n",
    "    if  v['time_to_fix_hours']>2 and v['n_slots_missed_due_to_upgrade_issues']>2 and v['public']==True:\n",
    "        bakers_fixed_within_5cycles_public['rolls_upgrade_cycle'].append(v['rolls_upgrade_cycle'])\n",
    "        bakers_fixed_within_5cycles_public['time_to_fix_hours'].append(v['time_to_fix_hours']) \n",
    "\n",
    "#Calculate time it took to fix private baker operations after initial failure following the upgrade\n",
    "bakers_fixed_within_5cycles_private = {\n",
    "                    'rolls_upgrade_cycle':[],\n",
    "                    'time_to_fix_hours':[]\n",
    "                    }\n",
    "for k,v in upgrade_results.items():\n",
    "    if  v['time_to_fix_hours']>2  and v['n_slots_missed_due_to_upgrade_issues']>2 and v['public']==False:\n",
    "        bakers_fixed_within_5cycles_private['rolls_upgrade_cycle'].append(v['rolls_upgrade_cycle'])\n",
    "        bakers_fixed_within_5cycles_private['time_to_fix_hours'].append(v['time_to_fix_hours']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate time it took to fix baker operations for bakers who voted\n",
    "bakers_voted_dict = {\n",
    "                    'rolls_upgrade_cycle':[],\n",
    "                    'time_to_fix_hours':[]\n",
    "                    }\n",
    "for k,v in upgrade_results.items():\n",
    "    if  v['time_to_fix_hours']>=0 and v['voted']==True:\n",
    "        bakers_voted_dict['rolls_upgrade_cycle'].append(v['rolls_upgrade_cycle'])\n",
    "        bakers_voted_dict['time_to_fix_hours'].append(v['time_to_fix_hours']) \n",
    "\n",
    "#Calculate time it took to fix baker operations for bakers who did not vote\n",
    "bakers_notvoted_dict = {\n",
    "                    'rolls_upgrade_cycle':[],\n",
    "                    'time_to_fix_hours':[]\n",
    "                    }\n",
    "for k,v in upgrade_results.items():\n",
    "    if  v['time_to_fix_hours']>=0 and v['voted']==False:\n",
    "        bakers_notvoted_dict['rolls_upgrade_cycle'].append(v['rolls_upgrade_cycle'])\n",
    "        bakers_notvoted_dict['time_to_fix_hours'].append(v['time_to_fix_hours']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reliability comparison before, during and after the upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "upgrade_results_table = pd.DataFrame(upgrade_results_pn)\n",
    "bakers_fixed_within_5cycles_public_df = pd.DataFrame(bakers_fixed_within_5cycles_public)\n",
    "bakers_fixed_within_5cycles_private_df = pd.DataFrame(bakers_fixed_within_5cycles_private)\n",
    "bakers_voted_dict_df = pd.DataFrame(bakers_voted_dict)\n",
    "bakers_notvoted_dict_df = pd.DataFrame(bakers_notvoted_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max.columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total number of bakers impacted by the upgrade\n",
    "active_bakers = upgrade_results_table.count()[\"address\"]\n",
    "active_bakers_share = upgrade_results_table.sum()[\"rolls_upgrade_cycle\"]\n",
    "\n",
    "impacted_bakers = upgrade_results_table[(upgrade_results_table['upgrade_ready']==False)].count()[\"address\"]\n",
    "impacted_bakers_share = upgrade_results_table[(upgrade_results_table['upgrade_ready']==False)].sum()[\"rolls_upgrade_cycle\"]\n",
    "\n",
    "ready_bakers = upgrade_results_table[(upgrade_results_table['upgrade_ready']==True)].count()[\"address\"]\n",
    "ready_bakers_share = upgrade_results_table[(upgrade_results_table['upgrade_ready']==True)].sum()[\"rolls_upgrade_cycle\"]\n",
    "\n",
    "bakers_cycle_after_upgrade_pn = {'active bakers':[active_bakers,active_bakers_share],\n",
    "                                 'all %':[\"{:.2f} %\".format(active_bakers/active_bakers*100),\"{:.2f} %\".format(active_bakers_share/active_bakers_share*100)],\n",
    "                                 'impacted bakers':[impacted_bakers,impacted_bakers_share],\n",
    "                                 'impacted %':[\"{:.2f} %\".format(impacted_bakers/active_bakers*100),\"{:.2f} %\".format(impacted_bakers_share/active_bakers_share*100)],\n",
    "                                 'upgrade ready bakers':[ready_bakers,ready_bakers_share],\n",
    "                                 'ready %':[\"{:.2f} %\".format(ready_bakers/active_bakers*100),\"{:.2f} %\".format(ready_bakers_share/active_bakers_share*100)]}\n",
    "bakers_cycle_after_upgrade_df = pd.DataFrame(bakers_cycle_after_upgrade_pn)\n",
    "bakers_cycle_after_upgrade_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_df = upgrade_results_table.sort_values(by=['time_to_fix_hours','rolls_upgrade_cycle'], ascending=False)\n",
    "sorted_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impacted_bakers_df = pd.DataFrame(upgrade_results_table[(upgrade_results_table['time_to_fix_hours']<0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Recovery time by Public vs Private bakers\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 8))\n",
    "                       \n",
    "x1 = bakers_fixed_within_5cycles_public_df.time_to_fix_hours\n",
    "y1 = bakers_fixed_within_5cycles_public_df.rolls_upgrade_cycle\n",
    "\n",
    "ax1 = plt.subplot(212)\n",
    "ax1.margins(0.05)   \n",
    "ax1.scatter( x1, y1, label='Public')   \n",
    "ax2 = bakers_fixed_within_5cycles_private_df.plot(kind='scatter', x='time_to_fix_hours', y='rolls_upgrade_cycle', color='red', grid=True, label='Private', ax=ax1)       \n",
    "\n",
    "ax1 = plt.xlabel('hours')\n",
    "ax1 = plt.ylabel('Balance in rolls (1 roll = 8000 tez)')\n",
    "ax1 = plt.title('Recovery time, Cycles 296-300')\n",
    "ax1 = plt.xlim(left=0, right=350)\n",
    "plt.legend(('Public','Private'))\n",
    "\n",
    "#Zoomed in to the cycle_upgrade\n",
    "\n",
    "ax3 = plt.subplot(221)\n",
    "ax3.scatter(x1, y1, label='Public')\n",
    "ax4 = bakers_fixed_within_5cycles_private_df.plot(kind='scatter', x='time_to_fix_hours', y='rolls_upgrade_cycle', color='red', grid=True, label='Private', ax=ax3)       \n",
    "ax3.set_title('Recovery time, Cycle 296 (excl. an outlier)')\n",
    "ax3 = plt.legend(('Public','Private'))\n",
    "ax3 = plt.ylim(top=500,bottom=0)\n",
    "ax3 = plt.xlim(left=0,right=72)\n",
    "ax3 = plt.xlabel('hours')\n",
    "ax3 = plt.ylabel('Balance in rolls (1 roll = 8000 tez)')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baker_rolls_list =[]\n",
    "income_json = requests.get(\"https://api.tzstats.com/tables/income?&cycle=%s&columns=rolls&limit=90429\" % cycle_upgrade)\n",
    "income = income_json.json()\n",
    "for baker_rolls in income:\n",
    "    baker_rolls_list.append(baker_rolls[0])\n",
    "num_rolls_cycle_upgrade = sum(baker_rolls_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovery_time_by_wealth = {'recovery_interval':[],'number_of_bakers':[],'number_of_rolls':[],'share_of_total_rolls':[]}\n",
    "\n",
    "interval_dict = {\n",
    "                'Upgrade ready':{ k:v for k,v in upgrade_results.items() \n",
    "                       if (v['upgrade_ready']==True)},\n",
    "                '0-3 days':{ k:v for k,v in upgrade_results.items() \n",
    "                       if  (v['time_to_fix_hours']>2 and v['time_to_fix_hours']<=72)},\n",
    "                '3-9 days':{ k:v for k,v in upgrade_results.items() \n",
    "                       if  (v['time_to_fix_hours']>72 and v['time_to_fix_hours']<=216)},\n",
    "                '9-15 days':{ k:v for k,v in upgrade_results.items() \n",
    "                       if  (v['time_to_fix_hours']>216 and v['time_to_fix_hours']<=360)},\n",
    "                 'Over 15 days':{ k:v for k,v in upgrade_results.items() \n",
    "                       if  v['time_to_fix_hours']<0}}\n",
    "\n",
    "for interval_k, interval_v in interval_dict.items():\n",
    "    recovery_time_by_wealth['recovery_interval'].append(interval_k)\n",
    "    recovery_time_by_wealth['number_of_bakers'].append(len(interval_v))\n",
    "    recovery_time_by_wealth['number_of_rolls'].append(sum([ v['rolls_upgrade_cycle'] for v in interval_v.values()]))\n",
    "    recovery_time_by_wealth['share_of_total_rolls'].append(sum([v['rolls_upgrade_cycle'] for v in interval_v.values()])/num_rolls_cycle_upgrade)\n",
    "\n",
    "recovery_time_by_wealth_impacted_bakers_df = pd.DataFrame(recovery_time_by_wealth)\n",
    "\n",
    "total_impacted_bakers = sum(recovery_time_by_wealth['number_of_bakers'])\n",
    "total_impacted_num_rolls = sum(recovery_time_by_wealth['number_of_rolls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create DataFrame from recovery_time_by_wealth dictionary\n",
    "recovery_time_by_wealth_df = pd.DataFrame(recovery_time_by_wealth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovery_time_by_wealth_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovery_time_by_vote = {'recovery_interval':[],'number_of_bakers_voted':[],'number_of_bakers_notvoted':[]}\n",
    "\n",
    "interval_dict_vote = {'Upgrade ready':{ k:v for k,v in upgrade_results.items() \n",
    "                       if  ((v['time_to_fix_hours']>=0 and v['time_to_fix_hours']<=2) \n",
    "                            and v['voted']==True)},\n",
    "                        'Impacted':{ k:v for k,v in upgrade_results.items() \n",
    "                       if  (((v['time_to_fix_hours']>2 and v['time_to_fix_hours']<=360) or v['time_to_fix_hours']<0)\n",
    "                             and v['voted']==True)}}\n",
    "\n",
    "interval_dict_novote = {'Upgrade ready':{ k:v for k,v in upgrade_results.items() \n",
    "                       if  ((v['time_to_fix_hours']>=0 and v['time_to_fix_hours']<=2) \n",
    "                            and v['voted']==False)},\n",
    "                        'Impacted':{ k:v for k,v in upgrade_results.items() \n",
    "                       if  (((v['time_to_fix_hours']>2 and v['time_to_fix_hours']<=360) or v['time_to_fix_hours']<0)\n",
    "                             and v['voted']==False)}}\n",
    "\n",
    "for interval_k, interval_v in interval_dict_vote.items():\n",
    "    recovery_time_by_vote['recovery_interval'].append(interval_k)\n",
    "    recovery_time_by_vote['number_of_bakers_voted'].append(len(interval_v))\n",
    "\n",
    "for interval_k, interval_v in interval_dict_novote.items():\n",
    "    recovery_time_by_vote['number_of_bakers_notvoted'].append(len(interval_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovery_time_by_vote_df = pd.DataFrame(recovery_time_by_vote)\n",
    "recovery_time_by_vote_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovery_time_by_vote = {'recovery_interval':[],'number_of_bakers':[]}\n",
    "\n",
    "interval_dict_vote = {'Upgrade ready voted':{ k:v for k,v in upgrade_results.items() \n",
    "                       if  ((v['time_to_fix_hours']>=0 and v['time_to_fix_hours']<=2) \n",
    "                            and v['voted']==True)},\n",
    "                        'Impacted voted':{ k:v for k,v in upgrade_results.items() \n",
    "                       if  (((v['time_to_fix_hours']>2 and v['time_to_fix_hours']<=360) or v['time_to_fix_hours']<0)\n",
    "                             and v['voted']==True)}}\n",
    "\n",
    "interval_dict_novote = {'Upgrade ready no vote':{ k:v for k,v in upgrade_results.items() \n",
    "                       if  ((v['time_to_fix_hours']>=0 and v['time_to_fix_hours']<=2) \n",
    "                            and v['voted']==False)},\n",
    "                        'Impacted no vote':{ k:v for k,v in upgrade_results.items() \n",
    "                       if  (((v['time_to_fix_hours']>2 and v['time_to_fix_hours']<=360) or v['time_to_fix_hours']<0)\n",
    "                             and v['voted']==False)}}\n",
    "\n",
    "for interval_k, interval_v in interval_dict_vote.items():\n",
    "    recovery_time_by_vote['recovery_interval'].append(interval_k)\n",
    "    recovery_time_by_vote['number_of_bakers'].append(len(interval_v))\n",
    "\n",
    "for interval_k, interval_v in interval_dict_novote.items():\n",
    "    recovery_time_by_vote['recovery_interval'].append(interval_k)\n",
    "    recovery_time_by_vote['number_of_bakers'].append(len(interval_v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recovery_time_by_vote_df = pd.DataFrame(recovery_time_by_vote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upgrade_status_dict = {'recovery_interval':[],'number_of_bakers':[]}\n",
    "interval_dict = {\n",
    "                'Upgrade ready':{ k:v for k,v in upgrade_results.items() \n",
    "                       if  (v['time_to_fix_hours']>=0 and v['time_to_fix_hours']<=2)},\n",
    "                'Impacted':{ k:v for k,v in upgrade_results.items() \n",
    "                       if  ((v['time_to_fix_hours']>2 and v['time_to_fix_hours']<=360) or v['time_to_fix_hours']<0)}}\n",
    "\n",
    "for interval_k, interval_v in interval_dict.items():\n",
    "    upgrade_status_dict['recovery_interval'].append(interval_k)\n",
    "    upgrade_status_dict['number_of_bakers'].append(len(interval_v)) \n",
    "\n",
    "upgrade_status_dict_df = pd.DataFrame(upgrade_status_dict)\n",
    "upgrade_status_dict_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade readiness by participation in the upgrade protocol elections\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib.pylab as pylab\n",
    "params = {'legend.fontsize': 'large',\n",
    "          'figure.figsize': (9, 5),\n",
    "         'axes.labelsize': 'large',\n",
    "         'axes.titlesize':'large',\n",
    "         'xtick.labelsize':'large',\n",
    "         'ytick.labelsize':'large'}\n",
    "pylab.rcParams.update(params)\n",
    "\n",
    "fig, ax = plt.subplots(subplot_kw=dict(aspect=\"equal\"))\n",
    "\n",
    "group_names= [\"Upgrade ready bakers\",\"Impacted bakers\"]\n",
    "group_size=upgrade_status_dict_df.number_of_bakers\n",
    "\n",
    "# Create colors\n",
    "a, b=[plt.cm.Greens, plt.cm.Reds]\n",
    " \n",
    "# First Ring (outside)\n",
    "mypie, _ = ax.pie(group_size, radius=1.2, wedgeprops=dict(width=0.5,edgecolor='white'), labels=group_names, colors=[a(0.7), b(0.7)] )\n",
    "\n",
    "data = [158,101,115,26]\n",
    "labels = [\"Ready, voted\",\"Ready, not voted\",\"Impacted, not voted\",\"Impacted, voted\"]\n",
    "\n",
    "# Create colors\n",
    "a, b=[plt.cm.Greens, plt.cm.Reds]\n",
    "\n",
    "\n",
    "def make_autopct(values):\n",
    "    def my_autopct(pct):\n",
    "        total = sum(values)\n",
    "        val = int(round(pct*total/100.0))\n",
    "        return '{p:.2f}%\\n  ({v:d})'.format(p=pct,v=val)\n",
    "    return my_autopct\n",
    "\n",
    "wedges, texts, autotexts = ax.pie(data, autopct=make_autopct(data),wedgeprops=dict(width=0.65, edgecolor='white'),\n",
    "                                  textprops=dict(color=\"w\"), colors=[a(0.5), a(0.6), b(0.6), b(0.4)])\n",
    "\n",
    "ax.legend(wedges, labels,\n",
    "          loc=\"center left\",\n",
    "          fontsize=12,\n",
    "          bbox_to_anchor=(1, 0, 0.5, 1))\n",
    "\n",
    "plt.setp(autotexts, size=9.5, weight=\"bold\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate number of missed endorsments and lost blocks 4 cycles before and 5 cycles after the upgrade\n",
    "income_upgrade_dict = {'cycle':[],\n",
    "                'n_blocks_lost':[],\n",
    "                'n_slots_missed':[],\n",
    "                'n_baking_rights':[],\n",
    "                'n_endorsing_rights':[],\n",
    "                'missed_baking_income':[],\n",
    "                'missed_endorsing_income':[]\n",
    "                }\n",
    "\n",
    "for cycle in cycle_upgrade_range:\n",
    "    income_upgrade_dict['cycle'].append(cycle)\n",
    "    \n",
    "    for metric in ['n_blocks_lost', 'n_slots_missed','n_baking_rights','n_endorsing_rights','missed_baking_income','missed_endorsing_income']:\n",
    "        metric_sum = sum([(baker_v[cycle][metric] if cycle in baker_v else 0) for baker_v in income_dict.values()])\n",
    "        income_upgrade_dict[metric].append(metric_sum)\n",
    "\n",
    "income_upgrade_dict_df = pd.DataFrame(income_upgrade_dict)\n",
    "income_upgrade_dict_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upgrade readiness and recovery time per number of bakers and their aggregate tez balance\n",
    "\n",
    "df = pd.DataFrame({'x' : [1, 2, 3, 4, 5]})\n",
    "x = df.x\n",
    "\n",
    "#x= recovery_time_by_wealth_df['recovery_interval']\n",
    "ax1 = plt.subplot(1,1,1)\n",
    "w = 0.3\n",
    "#plt.xticks(), will label the bars on x axis with the respective recovery interval\n",
    "plt.xticks(x + w /2, recovery_time_by_wealth_df['recovery_interval'])\n",
    "#plt.xticks(x + w /2, recovery_time_by_wealth_df['recovery_interval'])\n",
    "#generate bars for Y-left axis for shared X axis\n",
    "share =ax1.bar(x, recovery_time_by_wealth_df['number_of_rolls'], width=w, color='#FFD017', align='center')\n",
    "#The trick is to use two different axes that share the same x axis, we have used ax1.twinx() method.\n",
    "ax2 = ax1.twinx()\n",
    "#generate bars for Y-right axis for shared X axis\n",
    "bakers =ax2.bar(x + w, recovery_time_by_wealth_df['number_of_bakers'], width=w,color='#5DBB63',align='center')\n",
    "\n",
    "#Set the X axis label.\n",
    "ax1.set_xlabel('Recovery interval')\n",
    "#Set the Y-left axis label.\n",
    "ax1.set_ylabel('Aggregate amount at stake, in rolls')\n",
    "#Set the Y-right axis label.\n",
    "ax2.set_ylabel('Number of bakers')\n",
    "#To set the legend on the plot we have used plt.legend()\n",
    "plt.legend([share, bakers],['Aggregate amount at stake, in rolls (1 roll = 8000 tez)','Number of bakers'])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ready_bakers_share_total = recovery_time_by_wealth_impacted_bakers_df[(recovery_time_by_wealth_impacted_bakers_df.recovery_interval == 'Upgrade ready')].sum()['number_of_rolls']\n",
    "ready_bakers_total = recovery_time_by_wealth_impacted_bakers_df[(recovery_time_by_wealth_impacted_bakers_df.recovery_interval == 'Upgrade ready')].sum()['number_of_bakers']\n",
    "\n",
    "impacted_bakers_share_total = recovery_time_by_wealth_impacted_bakers_df[(recovery_time_by_wealth_impacted_bakers_df.recovery_interval != 'Upgrade ready')].sum()['number_of_rolls']\n",
    "impacted_bakers_total = recovery_time_by_wealth_impacted_bakers_df[(recovery_time_by_wealth_impacted_bakers_df.recovery_interval != 'Upgrade ready')].sum()['number_of_bakers']\n",
    "\n",
    "impacted_bakers_share_df = (recovery_time_by_wealth_impacted_bakers_df[(recovery_time_by_wealth_impacted_bakers_df.recovery_interval != 'Upgrade ready')]).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pie Chart for recovery_time_by_wealth_df\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# make figure and assign axis objects\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 5))\n",
    "fig.subplots_adjust(wspace=0)\n",
    "\n",
    "# pie chart parameters\n",
    "ratios = [impacted_bakers_total, ready_bakers_total]\n",
    "labels = ['Impacted bakers', 'Delphi ready bakers']\n",
    "colors = ['#FF2400','#00A86B']\n",
    "explode = [0.1, 0]\n",
    "ax1.pie(ratios, autopct='%1.1f%%', startangle=297, colors=colors,\n",
    "        labels=labels, explode=explode)\n",
    "ax1.set_title('Delphi ready vs impacted bakers', horizontalalignment='center')\n",
    "\n",
    "# bar chart parameters\n",
    "\n",
    "xpos = 0\n",
    "bottom = 0\n",
    "ratios = impacted_bakers_share_df.number_of_bakers\n",
    "width = 0.2\n",
    "colors = ['#fec8c2','#fd9c92','#fc7162', '#fc4531']\n",
    "labels = [impacted_bakers_share_df.number_of_bakers]\n",
    "\n",
    "for j in range(len(ratios)):\n",
    "    height = ratios[j]\n",
    "    ax2.bar(xpos, height, width, bottom=bottom, color=colors[j])\n",
    "    ypos = bottom + ax2.patches[j].get_height() / 2\n",
    "    bottom += height\n",
    "    ax2.text(xpos, ypos, (ax2.patches[j].get_height()),\n",
    "             ha='center')\n",
    "\n",
    "\n",
    "ax2.legend(impacted_bakers_share_df.recovery_interval,loc='upper right', bbox_to_anchor=(1.3, 0.8),\n",
    "           labelspacing=-2.5, frameon=False)\n",
    "ax2.set_title('Recovery time, number of bakers', horizontalalignment='left')\n",
    "ax2.axis('off')\n",
    "ax2.set_xlim(- 2.5 * width, 2.5 * width)\n",
    "\n",
    "\n",
    "# use ConnectionPatch to draw lines between the two plots\n",
    "# get the wedge data\n",
    "theta1, theta2 = ax1.patches[0].theta1, ax1.patches[0].theta2\n",
    "center, r = ax1.patches[0].center, ax1.patches[0].r\n",
    "bar_height = sum([item.get_height() for item in ax2.patches])\n",
    "\n",
    "# draw top connecting line\n",
    "x = r * np.cos(np.pi / 180 * theta2) + center[0]\n",
    "y = r * np.sin(np.pi / 180 * theta2) + center[1]\n",
    "con = ConnectionPatch(xyA=(-width / 2, bar_height), coordsA=ax2.transData,\n",
    "                      xyB=(x, y), coordsB=ax1.transData)\n",
    "con.set_color([0, 0, 0])\n",
    "con.set_linewidth(2)\n",
    "ax2.add_artist(con)\n",
    "\n",
    "# draw bottom connecting line\n",
    "x = r * np.cos(np.pi / 180 * theta1) + center[0]\n",
    "y = r * np.sin(np.pi / 180 * theta1) + center[1]\n",
    "con = ConnectionPatch(xyA=(-width / 2, 0), coordsA=ax2.transData,\n",
    "                      xyB=(x, y), coordsB=ax1.transData)\n",
    "con.set_color([0, 0, 0])\n",
    "ax2.add_artist(con)\n",
    "con.set_linewidth(2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pie Chart for recovery_time_by_wealth_df\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import ConnectionPatch\n",
    "import numpy as np\n",
    "\n",
    "# make figure and assign axis objects\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 5))\n",
    "fig.subplots_adjust(wspace=0)\n",
    "\n",
    "# pie chart parameters\n",
    "ratios = [impacted_bakers_share, ready_bakers_share]\n",
    "labels = ['Impacted share', 'Delphi ready share']\n",
    "colors = ['#FF2400','#57ADD2']\n",
    "explode = [0.1, 0]\n",
    "# rotate so that first wedge is split by the x-axis\n",
    "#angle = -180 * ratios[0]\n",
    "ax1.pie(ratios, autopct='%1.1f%%', startangle=340, colors=colors,\n",
    "        labels=labels, explode=explode)\n",
    "ax1.set_title('Delphi ready vs impacted share of total staked tez supply', horizontalalignment='center')\n",
    "\n",
    "\n",
    "# bar chart parameters\n",
    "\n",
    "xpos = 0\n",
    "bottom = 0\n",
    "ratios = impacted_bakers_share_df.share_of_total_rolls #[0.104550,0.007964,0.022994,0.001465]#\n",
    "width = 0.2\n",
    "colors = ['#fc4531','#fc7162','#fd9c92','#fec8c2']\n",
    "labels = [impacted_bakers_share_df.share_of_total_rolls]\n",
    "\n",
    "for j in range(len(ratios)):\n",
    "    height = ratios[j]\n",
    "    ax2.bar(xpos, height, width, bottom=bottom, color=colors[j])\n",
    "    ypos = bottom + ax2.patches[j].get_height() / 2\n",
    "    bottom += height\n",
    "    ax2.text(xpos, ypos, \"%0.1f%%\" % (ax2.patches[j].get_height() * 100),\n",
    "             ha='center')\n",
    "\n",
    "ax2.legend(impacted_bakers_share_df.recovery_interval,loc='upper right', bbox_to_anchor=(1.3, 0.8),\n",
    "           labelspacing=-2.5, frameon=False)\n",
    "ax2.set_title('Recovery time, pct. of total staked tez supply', horizontalalignment='left')\n",
    "ax2.axis('off')\n",
    "ax2.set_xlim(- 2.5 * width, 2.5 * width)\n",
    "\n",
    "# use ConnectionPatch to draw lines between the two plots\n",
    "# get the wedge data\n",
    "theta1, theta2 = ax1.patches[0].theta1, ax1.patches[0].theta2\n",
    "center, r = ax1.patches[0].center, ax1.patches[0].r\n",
    "bar_height = sum([item.get_height() for item in ax2.patches])\n",
    "\n",
    "# draw top connecting line\n",
    "x = r * np.cos(np.pi / 180 * theta2) + center[0]\n",
    "y = r * np.sin(np.pi / 180 * theta2) + center[1]\n",
    "con = ConnectionPatch(xyA=(-width / 2, bar_height), coordsA=ax2.transData,\n",
    "                      xyB=(x, y), coordsB=ax1.transData)\n",
    "con.set_color([0, 0, 0])\n",
    "con.set_linewidth(2)\n",
    "ax2.add_artist(con)\n",
    "\n",
    "# draw bottom connecting line\n",
    "x = r * np.cos(np.pi / 180 * theta1) + center[0]\n",
    "y = r * np.sin(np.pi / 180 * theta1) + center[1]\n",
    "con = ConnectionPatch(xyA=(-width / 2, 0), coordsA=ax2.transData,\n",
    "                      xyB=(x, y), coordsB=ax1.transData)\n",
    "con.set_color([0, 0, 0])\n",
    "ax2.add_artist(con)\n",
    "con.set_linewidth(2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "income_upgrade_dict = {'cycle':[],\n",
    "                'n_blocks_lost':[],\n",
    "                'n_slots_missed':[],\n",
    "                'n_baking_rights':[],\n",
    "                'n_endorsing_rights':[],\n",
    "                'missed_baking_income':[],\n",
    "                'missed_endorsing_income':[]\n",
    "                }\n",
    "\n",
    "for cycle in cycle_upgrade_range:\n",
    "    income_upgrade_dict['cycle'].append(cycle)\n",
    "    \n",
    "    for metric in ['n_blocks_lost', 'n_slots_missed','n_baking_rights','n_endorsing_rights','missed_baking_income','missed_endorsing_income']:\n",
    "        metric_sum = sum([(baker_v[cycle][metric] if cycle in baker_v else 0) for baker_v in income_dict.values()])\n",
    "        income_upgrade_dict[metric].append(metric_sum)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import host_subplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (8,5)\n",
    "\n",
    "host = host_subplot(111)\n",
    "host.set_xlabel(\"Cycle\")\n",
    "host.set_ylabel(\"Missed income, tez\")\n",
    "\n",
    "p1, = host.plot(income_upgrade_dict_df.cycle, income_upgrade_dict_df.missed_baking_income+income_upgrade_dict_df.missed_endorsing_income, label=\"Missed baking income\")\n",
    "\n",
    "leg = plt.legend()\n",
    "\n",
    "host.yaxis.get_label()\n",
    "host.set_title('Value not created before and after Delphi upgrade')\n",
    "x_line_annotation = 296\n",
    "x_text_annotation = 295.80\n",
    "host.axvline(x=x_line_annotation, linestyle='dashed', alpha=1, color='red')\n",
    "host.text(x=x_text_annotation, y=4000, s='Delphi upgrade', alpha=1, rotation='vertical', color='red')\n",
    "\n",
    "x_line_annotation = 296\n",
    "x_text_annotation = 294.80\n",
    "host.axvline(x=x_line_annotation, linestyle='dashed', alpha=1, color='red')\n",
    "host.text(x=x_text_annotation, y=16851, s='tez 16,851', alpha=1, rotation='horizontal', color='red')\n",
    "\n",
    "x_text_annotation = 298.6\n",
    "host.text(x=x_text_annotation, y=10800, s='tez 10,484', alpha=1, rotation='horizontal', color='red')\n",
    "\n",
    "x_text_annotation = 292.7\n",
    "host.text(x=x_text_annotation, y=3400, s='tez 3,348', alpha=1, rotation='horizontal', color='red')\n",
    "\n",
    "plt.xlim(left=292,right=300.5)\n",
    "host.set_xlim((292,300.5)) \n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (8,5)\n",
    "x = income_upgrade_dict_df['cycle']\n",
    "ax1 = plt.subplot(1,1,1)\n",
    "w = 0.3\n",
    "#plt.xticks(), will label the bars on x axis with the respective cycle IDs.\n",
    "plt.xticks(x + w /2, income_upgrade_dict_df['cycle'])\n",
    "#generate bars for Y-left axis for shared X axis\n",
    "slots =ax1.bar(x, income_upgrade_dict_df['n_slots_missed'], width=w, color='#296EB4', align='center')\n",
    "#The trick is to use two different axes that share the same x axis, we have used ax1.twinx() method.\n",
    "ax2 = ax1.twinx()\n",
    "#generate bars for Y-right axis for shared X axis\n",
    "blocks =ax2.bar(x + w, income_upgrade_dict_df['n_blocks_lost'], width=w,color='#F9A620',align='center')\n",
    "#Set the Graph title.\n",
    "plt.title('Missed endorsements & lost blocks before and after the Delphi upgrade')\n",
    "#Set the X axis label.\n",
    "ax1.set_xlabel('Cycle')\n",
    "#Set the Y-left axis label.\n",
    "ax1.set_ylabel('Endorsements missed')\n",
    "#Set the Y-right axis label.\n",
    "ax2.set_ylabel('Blocks lost')\n",
    "#To set the legend on the plot we have used plt.legend()\n",
    "plt.legend([slots, blocks],['Endorsements missed', 'Blocks lost'])\n",
    "\n",
    "x_line_annotation = 295.83\n",
    "x_text_annotation = 295.58\n",
    "ax1.axvline(x=x_line_annotation, linestyle='dashed', alpha=1, color='red')\n",
    "ax1.text(x=x_text_annotation, y=4000, s='Delphi upgrade', alpha=1, rotation='vertical', color='red')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
